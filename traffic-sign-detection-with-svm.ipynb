{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Traffic Sign Classification with SVM","metadata":{}},{"cell_type":"markdown","source":"## 1. Import Libraries","metadata":{}},{"cell_type":"code","source":"import time\nimport os \nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\n\nfrom skimage.transform import resize\nfrom skimage import feature\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:35.607915Z","iopub.execute_input":"2023-10-01T15:23:35.608421Z","iopub.status.idle":"2023-10-01T15:23:35.615835Z","shell.execute_reply.started":"2023-10-01T15:23:35.608378Z","shell.execute_reply":"2023-10-01T15:23:35.614672Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"annotations_dir = '/kaggle/input/traffic-sign/annotations'\nimg_dir = '/kaggle/input/traffic-sign/images'\n\nimg_lst = []\nlabel_lst = []","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:35.618539Z","iopub.execute_input":"2023-10-01T15:23:35.619210Z","iopub.status.idle":"2023-10-01T15:23:35.634868Z","shell.execute_reply.started":"2023-10-01T15:23:35.619181Z","shell.execute_reply":"2023-10-01T15:23:35.633710Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"for xml_file in os.listdir(annotations_dir):\n    xml_filepath = os.path.join(annotations_dir, xml_file)\n    tree = ET.parse(xml_filepath)\n    root = tree.getroot()\n    \n    folder = root.find('folder').text\n    img_filename = root.find('filename').text\n    img_filepath = os.path.join(img_dir, img_filename)\n    img = cv2.imread(img_filepath)\n    \n    for obj in root.findall('object'):\n        classname = obj.find('name').text\n        if classname != 'trafficlight':\n            xmin = int(obj.find('bndbox/xmin').text)\n            ymin = int(obj.find('bndbox/ymin').text)\n            xmax = int(obj.find('bndbox/xmax').text)\n            ymax = int(obj.find('bndbox/ymax').text)\n\n            object_img = img[ymin:ymax, xmin:xmax]\n            img_lst.append(object_img)\n            label_lst.append(classname)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:35.635944Z","iopub.execute_input":"2023-10-01T15:23:35.636765Z","iopub.status.idle":"2023-10-01T15:23:44.452811Z","shell.execute_reply.started":"2023-10-01T15:23:35.636735Z","shell.execute_reply":"2023-10-01T15:23:44.451520Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of objects: {len(img_lst)}\")\nprint(f'Classname: {list(set(label_lst))}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:44.454353Z","iopub.execute_input":"2023-10-01T15:23:44.454741Z","iopub.status.idle":"2023-10-01T15:23:44.459609Z","shell.execute_reply.started":"2023-10-01T15:23:44.454705Z","shell.execute_reply":"2023-10-01T15:23:44.458521Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Number of objects: 1074\nClassname: ['speedlimit', 'stop', 'crosswalk']\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_img(img):\n    if len(img.shape) > 2:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = img.astype(np.float32)\n    \n    resized_img = resize(\n        img,\n        output_shape = (32, 32),\n        anti_aliasing = True\n    )\n    \n    hog_feature = feature.hog(\n        resized_img,\n        orientations = 9,\n        pixels_per_cell = (8, 8),\n        cells_per_block = (2, 2),\n        transform_sqrt = True,\n        block_norm = \"L2\",\n        feature_vector= True\n    )\n    \n    return hog_feature","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:44.461719Z","iopub.execute_input":"2023-10-01T15:23:44.462513Z","iopub.status.idle":"2023-10-01T15:23:44.471556Z","shell.execute_reply.started":"2023-10-01T15:23:44.462486Z","shell.execute_reply":"2023-10-01T15:23:44.470512Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"img_features_lst = []\nfor img in img_lst:\n    hog_feature = preprocess_img(img)\n    img_features_lst.append(hog_feature)\n\nimg_features = np.array(img_features_lst)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:44.472904Z","iopub.execute_input":"2023-10-01T15:23:44.473419Z","iopub.status.idle":"2023-10-01T15:23:45.418412Z","shell.execute_reply.started":"2023-10-01T15:23:44.473392Z","shell.execute_reply":"2023-10-01T15:23:45.417314Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"print(f'Shape of the first image before preprocessing: {img_lst[0].shape}')\nprint(f'Shape of the first image after preprocessing: {img_features[0].shape}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.419692Z","iopub.execute_input":"2023-10-01T15:23:45.420069Z","iopub.status.idle":"2023-10-01T15:23:45.425308Z","shell.execute_reply.started":"2023-10-01T15:23:45.420037Z","shell.execute_reply":"2023-10-01T15:23:45.424238Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"Shape of the first image before preprocessing: (60, 60, 3)\nShape of the first image after preprocessing: (324,)\n","output_type":"stream"}]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(label_lst)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.426819Z","iopub.execute_input":"2023-10-01T15:23:45.427530Z","iopub.status.idle":"2023-10-01T15:23:45.439133Z","shell.execute_reply.started":"2023-10-01T15:23:45.427503Z","shell.execute_reply":"2023-10-01T15:23:45.438113Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"label_encoder.classes_","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.440475Z","iopub.execute_input":"2023-10-01T15:23:45.440770Z","iopub.status.idle":"2023-10-01T15:23:45.453686Z","shell.execute_reply.started":"2023-10-01T15:23:45.440745Z","shell.execute_reply":"2023-10-01T15:23:45.452750Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"array(['crosswalk', 'speedlimit', 'stop'], dtype='<U10')"},"metadata":{}}]},{"cell_type":"code","source":"encoded_labels","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.454700Z","iopub.execute_input":"2023-10-01T15:23:45.454952Z","iopub.status.idle":"2023-10-01T15:23:45.466911Z","shell.execute_reply.started":"2023-10-01T15:23:45.454931Z","shell.execute_reply":"2023-10-01T15:23:45.465951Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"array([1, 1, 1, ..., 1, 1, 1])"},"metadata":{}}]},{"cell_type":"code","source":"random_state = 0\ntest_size = 0.3\nis_shuffle = True\n\nX_train, X_val, y_train, y_val = train_test_split(\n    img_features, encoded_labels,\n    test_size=test_size,\n    random_state=random_state,\n    shuffle=is_shuffle\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.468438Z","iopub.execute_input":"2023-10-01T15:23:45.468832Z","iopub.status.idle":"2023-10-01T15:23:45.478483Z","shell.execute_reply.started":"2023-10-01T15:23:45.468797Z","shell.execute_reply":"2023-10-01T15:23:45.477508Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"clf = SVC(\n    kernel='rbf',\n    random_state=random_state,\n    probability=True,\n    C=0.5\n)\n\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.479763Z","iopub.execute_input":"2023-10-01T15:23:45.480199Z","iopub.status.idle":"2023-10-01T15:23:45.620427Z","shell.execute_reply.started":"2023-10-01T15:23:45.480166Z","shell.execute_reply":"2023-10-01T15:23:45.619342Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"SVC(C=0.5, probability=True, random_state=0)","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.5, probability=True, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.5, probability=True, random_state=0)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = clf.predict(X_val)\nscore = accuracy_score(y_pred, y_val)\n\nprint(f'Evaluation results on val set Accuracy: {score}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.623129Z","iopub.execute_input":"2023-10-01T15:23:45.623537Z","iopub.status.idle":"2023-10-01T15:23:45.647569Z","shell.execute_reply.started":"2023-10-01T15:23:45.623509Z","shell.execute_reply":"2023-10-01T15:23:45.646672Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Evaluation results on val set Accuracy: 0.9814241486068112\n","output_type":"stream"}]},{"cell_type":"code","source":"input_img = img_lst[1]\nnormalized_img = preprocess_img(input_img)\ny_pred = clf.predict([normalized_img])[0]\nprint(f'Normal prediction: {y_pred}')\ny_pred_prob = clf.predict_proba([normalized_img])\nprediction = np.argmax(y_pred_prob)\ny_pred_prob = [f'{p:.10f}' for p in y_pred_prob[0]]\n\nprint(f'Probability of each class: {y_pred_prob}')\nprint(f'Class with highest probability: {prediction}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.652244Z","iopub.execute_input":"2023-10-01T15:23:45.652600Z","iopub.status.idle":"2023-10-01T15:23:45.663043Z","shell.execute_reply.started":"2023-10-01T15:23:45.652572Z","shell.execute_reply":"2023-10-01T15:23:45.661809Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Normal prediction: 1\nProbability of each class: ['0.0000212628', '0.9949362325', '0.0050425046']\nClass with highest probability: 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# II.Traffic Sign Detection","metadata":{}},{"cell_type":"code","source":"def sliding_window(img, window_sizes, stride, scale_factor):\n    img_height, img_width = img.shape[:2]\n    windows = []\n    for window_size in window_sizes:\n        window_width, window_height = window_size\n        for ymin in range(0, img_height - window_height + 1, stride):\n            for xmin in range(0, img_width - window_width + 1, stride):\n                xmax = xmin + window_width\n                ymax = ymin + window_height\n                \n                windows.append([xmin, ymin, xmax, ymax])\n    \n    return windows","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.664323Z","iopub.execute_input":"2023-10-01T15:23:45.664663Z","iopub.status.idle":"2023-10-01T15:23:45.672983Z","shell.execute_reply.started":"2023-10-01T15:23:45.664624Z","shell.execute_reply":"2023-10-01T15:23:45.672161Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def pyramid(img, scale=0.8, min_size= (30, 30)):\n    acc_scale = 1.0 \n    pyramid_imgs = [(img, acc_scale)]\n    \n    i = 0\n    while True:\n        acc_scale = acc_scale * scale\n        h = int(img.shape[0] * acc_scale)\n        w = int(img.shape[1] * acc_scale)\n        if h < min_size[1] or w < min_size[0]:\n            break\n        img = cv2.resize(img,(w,h))\n        pyramid_imgs.append((img, acc_scale*(scale**i)))\n        i += 1\n        \n    return pyramid_imgs","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.674197Z","iopub.execute_input":"2023-10-01T15:23:45.674714Z","iopub.status.idle":"2023-10-01T15:23:45.686468Z","shell.execute_reply.started":"2023-10-01T15:23:45.674686Z","shell.execute_reply":"2023-10-01T15:23:45.685397Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"img_dir = \"/kaggle/input/traffic-sign/images\"\nimg_filename_lst = os.listdir(img_dir)[:20]\nconf_threshold = 0.8 \nstride = 12\nwindow_sizes = [\n    (32, 32),\n    (64, 64),\n    (128, 128)\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.687567Z","iopub.execute_input":"2023-10-01T15:23:45.688344Z","iopub.status.idle":"2023-10-01T15:23:45.708688Z","shell.execute_reply.started":"2023-10-01T15:23:45.688317Z","shell.execute_reply":"2023-10-01T15:23:45.707808Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"for img_filename in img_filename_lst:\n    start_time = time.time()\n    img_filepath = os.path.join(img_dir, img_filename)\n    bboxes = []\n    img = cv2.imread(img_filepath)\n    pyramid_imgs = pyramid(img)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.709969Z","iopub.execute_input":"2023-10-01T15:23:45.710467Z","iopub.status.idle":"2023-10-01T15:23:45.907956Z","shell.execute_reply.started":"2023-10-01T15:23:45.710439Z","shell.execute_reply":"2023-10-01T15:23:45.907058Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"for pyramid_img_info in pyramid_imgs:\n    pyramid_img, scale_factor = pyramid_img_info\n    window_lst = sliding_window(\n        pyramid_img, \n        window_sizes = window_sizes,\n        stride = stride,\n        scale_factor = scale_factor\n    )\n    \n    for window in window_lst:\n        xmin, ymin, xmax, ymax = window\n        object_img = pyramid_img[ymin:ymax, xmin:xmax]\n        preprocessed_img = preprocess_img(object_img)\n        scaler.fit(preprocessed_img.reshape(1,-1))\n        normalized_img = scaler.transform([preprocessed_img])[0]\n        decision = clf.predict_proba([normalized_img])[0]\n        if np.all(decision < conf_threshold):\n            continue\n        else:\n            predict_id = np.argmax(decision)\n            conf_score = decision[predict_id]\n            xmin = int(xmin / scale_factor)\n            ymin = int(ymin / scale_factor)\n            xmax = int(xmax / scale_factor)\n            ymax = int(ymax / scale_factor)\n            bboxes.append(\n                [xmin, ymin, xmax, ymax, predict_id, conf_score]\n            )","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:45.909305Z","iopub.execute_input":"2023-10-01T15:23:45.909703Z","iopub.status.idle":"2023-10-01T15:23:50.582629Z","shell.execute_reply.started":"2023-10-01T15:23:45.909668Z","shell.execute_reply":"2023-10-01T15:23:50.581503Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"def compute_iou(bbox, bboxes, bbox_area, bboxes_area):\n    xxmin = np.maximum(bbox[0], bboxes[:, 0])\n    yymin = np.maximum(bbox[1], bboxes[:, 1])\n    xxmax = np.minimum(bbox[2], bboxes[:, 2])\n    yymax = np.minimum(bbox[3], bboxes[:, 3])\n    \n    w = np.maximum(0, xxmax - xxmin + 1)\n    h = np.maximum(0, yymax - yymin + 1)\n    \n    intersection = w * h\n    iou = intersection / (bbox_area + bboxes_area - intersection)\n    \n    return iou","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:23:50.584132Z","iopub.execute_input":"2023-10-01T15:23:50.584523Z","iopub.status.idle":"2023-10-01T15:23:50.592168Z","shell.execute_reply.started":"2023-10-01T15:23:50.584484Z","shell.execute_reply":"2023-10-01T15:23:50.591024Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"def nms(bboxes, iou_threshold=0.9):\n    if not bboxes:\n        return []\n    \n    scores = np.array([bbox[5] for bbox in bboxes])\n    sorted_indices = np.argsort(scores)[::-1]\n    \n    xmin = np.array([bbox[0] for bbox in bboxes])\n    ymin = np.array([bbox[1] for bbox in bboxes])\n    xmax = np.array([bbox[2] for bbox in bboxes])\n    ymax = np.array([bbox[3] for bbox in bboxes])\n    \n    areas = (xmax - xmin + 1) * (ymax - ymin + 1)\n    keep = []\n    \n    while sorted_indices.size > 0:\n        i = sorted_indices[0]\n        keep.append(i)\n        \n        iou = compute_iou(\n            [xmin[i], ymin[i], xmax[i], ymax[i]],\n            np.array(\n                [\n                    xmin[sorted_indices[1:]],\n                    ymin[sorted_indices[1:]],\n                    xmax[sorted_indices[1:]],\n                    ymax[sorted_indices[1:]]]\n                    ).T,\n            areas[i],\n            areas[sorted_indices[1:]]\n        )\n        idx_to_keep = np.where(iou <= iou_threshold)[0]\n        sorted_indices = sorted_indices[idx_to_keep + 1]\n    \n    return [bboxes[i] for i in keep]","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:28:48.262227Z","iopub.execute_input":"2023-10-01T15:28:48.263371Z","iopub.status.idle":"2023-10-01T15:28:48.271408Z","shell.execute_reply.started":"2023-10-01T15:28:48.263336Z","shell.execute_reply":"2023-10-01T15:28:48.270661Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def visualize_bbox(img, bboxes, label_encoder):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    for box in bboxes:\n        xmin, ymin, xmax, ymax, predict_id, conf_score = box\n        \n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        \n        classname = label_encoder.inverse_transform([predict_id])[0]\n        label = f'{classname} {conf_score:.2f}'\n        \n        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n        \n        cv2.rectangle(img, (xmin, ymin - 20), (xmin + w, ymin), (0,255,0), -1)\n        cv2.putText(img, label, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 1)\n        \n        plt.imshow(img)\n        plt.axis('off')\n        plt.show()\n        break","metadata":{"execution":{"iopub.status.busy":"2023-10-01T15:29:25.441825Z","iopub.execute_input":"2023-10-01T15:29:25.442185Z","iopub.status.idle":"2023-10-01T15:29:25.450448Z","shell.execute_reply.started":"2023-10-01T15:29:25.442159Z","shell.execute_reply":"2023-10-01T15:29:25.449448Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}